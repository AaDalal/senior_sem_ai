{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting (classifier and regression)\n",
    "\n",
    "### Decision trees\n",
    "* 2 decisions: what feature to split and where to split\n",
    "* Root node = entire dataset; decision node = when the data is split; leaf = where the data is categorized/regressed\n",
    "* Typically requires descretized data so that you can compare score of a split location without having to calculate the score for an inordinate number of possible split locations.\n",
    "* Typical splitting criteria are entropy decrease, information gain, or similarity gain among the groups\n",
    "  * Similarity score can be calculated as $(sum\\_of\\_ residuals)^2/(num\\_residuals + \\lambda)$ where residuals are from the mean of the root node, which is the prediction. When there are values on both sides of this prediction, they will cancel and make this similarity score lower (Starmer).\n",
    "  * The similarity gain is: (similarity of the left node) + (similarity of right node) - (similarity of root) (Starmer).\n",
    "  * The similarity gain is calculated for all possible splits and then the highest gain is selected\n",
    "* Stopping criteria is typically that a less than a minimum number of data at a node converts it to a leaf\n",
    "* Pruning is done to stop overfitting by removing nodes that do not contribute to the reduction of the cost function (ie accuracy) (Seif)\n",
    "* Cost function is typically $\\sum (Y-\\hat{Y})^2$ for regression or Gini index function $\\sum (p_{k}*(1-p_{k}))$ where $p_{k}$ is the proportion of a leaf node's samples of each class (Seif)\n",
    "* The leaf nodes are assigned based on the class what most of the samples in the node\n",
    "\n",
    "### Bagging\n",
    "* Sample m times with replacement from the original train dataset (of size $n$), with each of the $m$ samples having $n'$ size and train 1 decision tree on each sample\n",
    "  * Typically $n=n'$. According to Aslam, Popa, and Rivest there is a $1-1/e$ (63%) of the original training dataset being represented in each sample in this case.\n",
    "  * Replacement occurs after each one that you pick. E.g., if you have 3 marbles in your dataset (1 red, 1 blue, 1 green) and are choosing two, if you pick red once, you may pick it again to make a sample of 2 red marbles.\n",
    "* Bagging works because it equalzes the \"leverage\" of each example to stop outliers or particular examples from having too great an impact. It does this because those high leverage examples are few in number, so they are often left out of bagging samples and therefore many models are produced without the effect of these high leverage examples, therefore the impact of the sample is reduced.\n",
    "* Each individual tree is combined so that it can \"vote\" on the ultimate prediction\n",
    "\n",
    "### Boosting\n",
    "* Construct decision trees to ensemble in a forest like in boosting\n",
    "* However, the output of the previous tree is used in the next. Specifically, the ones that were wrong from the previous tree are weighted higher as the input the next one\n",
    "* Gradient boosting is the method that measures which samples contributed more to the cost function by taking the derivative of the loss funciton with respect to the sample. This is similar to how it was used in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Resources</center>\n",
    "\n",
    "http://cs229.stanford.edu/notes/cs229-notes-ensemble.pdf\n",
    "\n",
    "http://people.csail.mit.edu/rivest/pubs/APR07.pdf\n",
    "\n",
    "http://www.math.univ-toulouse.fr/~agarivie/Telecom/apprentissage/articles/BaggingML.pdf\n",
    "\n",
    "https://towardsdatascience.com/a-guide-to-decision-trees-for-machine-learning-and-data-science-fe2607241956\n",
    "\n",
    "https://www.youtube.com/watch?v=OtD8wVaFm6E\n",
    "\n",
    "Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 785â€“794). ACM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic decision trees\n",
    "\n",
    "# for all values of the split calculate cross-entropy to the version where there is no split\n",
    "# If the maximum cross entropy is above a threshhold, split and split at that value\n",
    "# To find the most optimal value, take the mean if the two biggest cross entropy values are sequential\n",
    "# The two distributions being compared are the true distribution for a sample vs the one predicted by the decision tree\n",
    "# One simple application is do do the cross entropy class by class\n",
    "\n",
    "from math import log2\n",
    "import numpy as np\n",
    "\n",
    "def cross_entropy(p, q):\n",
    "    # p & q are distributions representing the same thing\n",
    "    # Cross entropy measures the average total number of bits from Q instead of P\n",
    "    # This can be broken down into the number of bits to represent P and the difference in bits from P to Q\n",
    "        # Therefore can say: E(P, Q) = E(P) + KL(P||Q) where KL is the extra bits \n",
    "    assert p.ndim == 1\n",
    "    assert p.ndim == q.ndim\n",
    "    # Elementwise sum of p_i * log2(q_i) for all i\n",
    "    return -1*np.sum(p*log2(q))\n",
    "\n",
    "def simple_dtrees(X, y, termination_tolerance, num_alternate_tresholds, alternate_threshold_step):\n",
    "    # The decision criteria are stored in a matrix, where each row is the depth and each column is the \n",
    "    # Each decision criteria contains 3 items, previous decision status (0 or 1), \n",
    "    criteria = [[[]]]\n",
    "    cat_dict = {}\n",
    "    while total_ce < termination_tolerance:\n",
    "        for x in X:\n",
    "            for layer in criteria:\n",
    "                pass\n",
    "                \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20)\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng()\n",
    "X = np.array(samples[0], copy = True)\n",
    "print(X.shape)\n",
    "chosen = rng.choice(X, size = 50, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ads_df = pd.read_csv(pathlib.Path(r'./datasets/Introduction to Statistical Learning with Applications in R/Advertising.csv'))\n",
    "ads_df.head()\n",
    "ads_df = ads_df[ads_df.columns[1:]]\n",
    "ads_df.head()\n",
    "data = ads_df.values[:, :-1]; label = ads_df.values[:,-1]\n",
    "data_train, data_test, label_train, label_test = train_test_split(data, label ,test_size = .6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  2.4812375084350458\n",
      "avg sales for comparison:  14.0225\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from math import sqrt\n",
    "\n",
    "dtrain = xgb.DMatrix(data_train, label=label_train)\n",
    "\n",
    "# All parameters: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "# eta: step size\n",
    "# gamma: how much loss reduction to get division\n",
    "# max_depth: how many layers of nodes past the root the tree can have\n",
    "param = {'max_depth':2, 'eta':1,}\n",
    "num_round = 2\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "dtest = xgb.DMatrix(data_test)\n",
    "labelpred = bst.predict(dtest)\n",
    "\n",
    "print('RMSE: ', sqrt(sum((label_test - labelpred)**2)/len(label_test)))\n",
    "print('avg sales for comparison: ', np.mean(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Type of Data\n",
    "* Categorical or continuous\n",
    "  * Note: the output space for regression decision trees is not continuous like with other methods, such as sigmoid regression\n",
    "\n",
    "### 2. Use Case\n",
    "* When it is acceptable not to have a continuous output space\n",
    "\n",
    "### 3. Application\n",
    "* Almost any regression or classification task satisfying the above\n",
    "* Especially when training time or possible overfitting is not relevant\n",
    "\n",
    "### 4. Basic Concept\n",
    "* Gradient boosting uses multiple decision trees trained sequentially, where the examples that were poorly categorized in the previous tree are weighted higher in the next tree.\n",
    "\n",
    "### 5. Assumptions\n",
    "* Worthwhile splits exist\n",
    "\n",
    "### 6. Existing solutions\n",
    "* XGBoost, LighGBM (which is a method that is more efficient in its sample dropout stage by not dropping out samples with high gradient (that we have a lot to learn from)), catboost\n",
    "\n",
    "### 7. Strengths and Weaknesses\n",
    "#### Strengths\n",
    "* Highly accurate\n",
    "* Not as data or resource hungry as deep learning algorithms\n",
    "\n",
    "#### Weaknesses\n",
    "* Prone to overfit\n",
    "* Non-continuous output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('tf2': conda)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
