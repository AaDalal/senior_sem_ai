{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "### Bayes'  Theorem\n",
    "* Think about making a prediction, given a certain set of information\n",
    "* You have **Prior** beliefs, which is what you would assume without the data. Typically, this is the strict proportion of samples that have a certain category.\n",
    "* You have the **:ikelihood** function, which is the probability of observing the data given the observation. This is seperate from the posterior, \n",
    "* You have the **Normalization**, which is how likely you are to observe the data at all. You must divide this out because you want to prove that it is \"given\" the data.\n",
    "\n",
    ">$P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}$\n",
    "0. $P(Y|X) = Posterior$\n",
    "1. $P(Y) or P(Y|E)= Prior$\n",
    "2. $P(X|Y) = Likelihood$\n",
    "3. $P(D|E) = Normalization$\n",
    "\n",
    "### The \"Naive\" assumption\n",
    "* To be precise, $P(X|Y)$ might require that you already observed a case with the right mix of data. This might not be true when you have many features to your data which each have many options.\n",
    "* Instead, you can naively assume that all of the features of the data are unrelated. The result is that the likelihood can be expressed as the simple multiple of the probabilities of all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Resources</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import sklearn as skl\n",
    "\n",
    "# Naive bayes expressed in code\n",
    "def naive_bayes(y, x, categories, data):\n",
    "    y = np.array(y)\n",
    "    x = np.array(x)\n",
    "    \n",
    "    posteriors = {}\n",
    "    likelihood_all = []\n",
    "    prior_all = []\n",
    "    normalization_all = []\n",
    "    \n",
    "    for category in categories:\n",
    "        \n",
    "        given_cat = (y == category)\n",
    "        prior = np.count_nonzero(given_cat) / len(y)\n",
    "        \n",
    "        x_given_cat = x[given_cat]\n",
    "        likelihood = 1\n",
    "        for i in range(len(data)):\n",
    "            feature = x_given_cat[:, i]\n",
    "            likelihood *= np.count_nonzero(feature == data[i])/np.count_nonzero(given_cat)            \n",
    "        \n",
    "        normalization = 1\n",
    "        for i in range(len(data)):\n",
    "            feature = x[:, i]\n",
    "            normalization *= np.count_nonzero(feature == data[i])/np.count_nonzero(y)\n",
    "        \n",
    "        likelihood_all.append(likelihood)\n",
    "        prior_all.append(prior)\n",
    "        normalization_all.append(normalization)\n",
    "        \n",
    "        posteriors[category] = prior*likelihood/normalization\n",
    "    return (posteriors, prior_all, likelihood_all, normalization_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to use Naive bayes with Reuters dataset\n",
    "\n",
    "from tensorflow.keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words = 10000)\n",
    "\n",
    "def vectorizer(sequences, dimension=10000):\n",
    "    # Convert lists of different sizes to boolean vectors with length equal to the total number of words\n",
    "    vectorized = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        vectorized[i, sequence] = 1\n",
    "    return vectorized\n",
    "\n",
    "x_train = vectorizer(train_data)\n",
    "x_test = vectorizer(test_data)\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "categories = np.arange(max(train_labels)+1)\n",
    "\n",
    "posteriors, priors, likelihoods, normalizations = naive_bayes(y_train, x_train, categories, x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "\n",
    "plt.plot(np.arange(len(priors)), priors, c = 'r', label = 'Prior')\n",
    "plt.plot(np.arange(len(likelihoods)), likelihoods, c = 'o', label = 'Likelihood')\n",
    "plt.plot(np.arange(len(normalization)), normalization, c = 'o', label = 'Normalization')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel(\"Value of Component of Bayes' Theorem\")\n",
    "plt.title('Relationship between Value of Prior, Likelihood and Normalization and Category')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('tf': conda)",
   "language": "python",
   "name": "python37664bittfcondaf0a0b333407b4c12bfca13c48d28a585"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
