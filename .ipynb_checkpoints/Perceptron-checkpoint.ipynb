{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron Neural Network\n",
    "\n",
    "### Vocabulary\n",
    "* Artificial Intelligence: \"automate intellectual tasks performed by humans\" (Chollet, 2018, p. 4)\n",
    "  * The originiation of an idea from a computer is not needed. E.g., symbolic AI uses hardcoded instructions (Chollet, 2018, p. 4)\n",
    "* Machine Learning: a subset of artificial intelligence where examples and expected outputs are used to *train* a computer to the rules (Chollet, 2018)\n",
    "  * In essence, the machine learning algorithm creates a \"new representation of the data\" (Chollet, 2018). Just as crucially, it does this by learning from it's previous representation to form a better one\n",
    "* Deep learning: a subset of machine learning where there are multiple layers of learning (Chollet, 2018)\n",
    "  * often times, this means that the model can use much simpler transformations, therefore deep learning is often more versatile than other methods\n",
    "* Tensor: a matrix of data where each axis is a different characteristic \n",
    "* Data type: the type of data in the tensor\n",
    "  * *Note: not the same as how the data is encoded within a tensor*\n",
    "* Rank: the number of axes (what we would traditionally think of as number of unique dimensions)\n",
    "* Shape: the number of dimensions along each axis\n",
    "  * eg. for image data, the shape could be described as (samples, height of image, width, color_depth)\n",
    "* Broadcasting: what occurs when two tensors of unequal shapes are operating together. \n",
    "  * *Note: broadcasting does not require same number of dimensions, but rather the same size of dimensions*\n",
    "* Dot product/Matrix multiplication: sum of element wise product for vectors; when matrices are multiplied, matrix multiplication is applied by applying dot product\n",
    "* Weights: the parameters of the neural network that are changed as it learns\n",
    "* Back Propogation: the general process by which the weights of a neural netowrk\n",
    "* Gradient: derivative for multiple variables. The idea is that the traditional derivative lacks the information necescary to sum up the data when there are 3+ variables. You can no longer sum up the changes occuring with just a scalar - you need a vector, hence you need a gradient\n",
    "\n",
    "### Basic Idea of a Perceptron\n",
    "* A perceptron is as to a artificial neural network as a neuron is to a real neural network\n",
    "* The perceptron typically consists of 3 things:\n",
    "  1. An input function that takes the inputs from all other connected neurons and combines them, often by simple summation\n",
    "  2. A nonlinear activation function\n",
    "  3. An output\n",
    "* The non-linearity of the step function is vital because otherwise the neural network would simply be linearly transforming data\n",
    "\n",
    "### Backpropogation\n",
    "A neural network has an **loss function** that evaluates how it has done as a whole. Using the data the loss provides, the **optimizer** alters the weights of the neural network. The way that this optimizer works is \n",
    "\n",
    "## Categorical (discrete) Data\n",
    "* Among the most common methods of categorizing data is by having the neural network output a value between 0 and 1 for each category\n",
    "* The highest value is typically chosen as the predicted category for the data\n",
    "* However, fuzzy sets, where how much a particular observation matches with the data pose an alternative method that benefits from the precision of the continuous data\n",
    "\n",
    "## Pros and Cons\n",
    "* Strength: extremely parallelized processing (greater speed), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Resources</center>\n",
    "http://library.isical.ac.in:8080/jspui/bitstream/10263/4569/1/308.pdf\n",
    "\n",
    "Chollet, F. (2018). Deep learning with Python. Manning Publications Co\n",
    "\n",
    "https://numpy.org/doc/stable/user/basics.broadcasting.html\n",
    "\n",
    "https://keras.io/guides/sequential_model/\n",
    "\n",
    "https://www.youtube.com/watch?v=GlcnxUlrtek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n"
     ]
    }
   ],
   "source": [
    "# For simplicity's sake, we will use a dataset where each sample can be represented as a vector\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(\n",
    "    path=\"boston_housing.npz\", test_split=0.2, seed=113\n",
    ")\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215.079538125\n",
      "0.000966899999184534\n",
      "215.079538125\n",
      "0.31423190000350587\n",
      "[205.07953812 252.07953812 185.07953812 207.07953812 193.07953812\n",
      " 202.07953812  96.07953812 221.07953812 216.07953812 111.07953812\n",
      " 327.07953812 297.07953812 174.07953812]\n"
     ]
    }
   ],
   "source": [
    "# Most layers in our network will use the simple model for a perceptron outlined by Chollet (2018, p. 38)\n",
    "# output = relu(dot(W, input) + b)\n",
    "# W and b are initially assigned randomly\n",
    "# The following code in this cell will be based on Chollet (2018)\n",
    "\n",
    "import timeit\n",
    "\n",
    "# For the purpose of understanding, I have constructed this function\n",
    "def naive_vector_dot(a,b):\n",
    "    assert a.ndim == 1\n",
    "    assert b.ndim == 1\n",
    "    assert b.size == a.size\n",
    "    \n",
    "    y = 0\n",
    "    \n",
    "    for i in range(a.size):\n",
    "        # Add up the elementwise product\n",
    "        y += a[i]*b[i]\n",
    "    \n",
    "    return y\n",
    "\n",
    "random_linear_multiples = np.random.randint(-128, high = 128, size = x_train.shape[1])/128\n",
    "start = timeit.default_timer()\n",
    "print(naive_vector_dot(x_train[0], random_linear_multiples))\n",
    "\n",
    "end = timeit.default_timer()\n",
    "print(end-start)\n",
    "\n",
    "# naive_vector_dot is the does the same thing to the vectors of np.dot(a,b); \n",
    "# however np.dot() is faster and can handle more axes\n",
    "\n",
    "start0 = timeit.default_timer()\n",
    "print(np.dot(x_train[0], random_linear_multiples))\n",
    "end0 = timeit.default_timer()\n",
    "print(end0-start0)\n",
    "\n",
    "# Add in scalar weights \n",
    "linear = np.dot(x_train[0], random_linear_multiples) + np.random.randint(-128, 128, size = x_train.shape[1])\n",
    "\n",
    "# Relu (also known as linear rectifier): max(0, linear)\n",
    "\n",
    "perceptron_out = np.maximum(0, linear)\n",
    "print(perceptron_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "# Loss function: output - relu(dot(input, w)+b)\n",
    "# gradient = derivative of a tensor, which ends up being a tensor of derivatives\n",
    "# Typically, we would try to find the minimum by determing the weight values where the loss function has the lowest derivative, but this is unviable because it is too computationally expensive\n",
    "# Instead we \"ride the gradient\" to where a local minimum of the loss function is\n",
    "\n",
    "# ReLu has a sharp corner (due to it being a piecewise function where y<0 for x<0 and y=x for x>0) therefore is not differentiable\n",
    "# However though it is not generally differentiable. it is truly differentiable at all x except x=0\n",
    "# Therefore it has a derivative of 1 when input > 0, derivatvie 0 when input<0. Because this is an elementwise operation, a derivative will suffice\n",
    "\n",
    "# XXX: include the bias after doing weights only\n",
    "\n",
    "import math\n",
    "\n",
    "def z_score(x_train, x_test):\n",
    "    # Note that all transformation on the data are based on the training data\n",
    "    # The goal is to make sure that you don't accidentally train on any aspects of the test data\n",
    "    mean = x_train.mean(axis = 0)\n",
    "    std = x_train.std(axis = 0)\n",
    "    \n",
    "    x_train_norm = (x_train - mean) / std\n",
    "    x_test_norm = (x_test - mean) / std\n",
    "    \n",
    "    return (x_train_norm, x_test_norm)\n",
    "\n",
    "def relu(z):\n",
    "    z[z<0] = 0\n",
    "    return z\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + math.e**(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    sig = sigmoid(z)\n",
    "    return sig*(1-sig)\n",
    "    \n",
    "def linear(z):\n",
    "    return z\n",
    "\n",
    "def linear_prime(z):\n",
    "    return 1\n",
    "\n",
    "class neural_network:\n",
    "    def __init__(self, step_size):\n",
    "        self.y_hat = None\n",
    "        self.step_size = step_size\n",
    "        \n",
    "    class layer(): \n",
    "        \"\"\"\n",
    "        Parameters\n",
    "         ----------\n",
    "        activation: \n",
    "        \"\"\"\n",
    "        def __init__(self, num_nodes, activation = None, activation_prime = None, kernel = None, bias = None):\n",
    "            self.num_nodes = num_nodes\n",
    "            self.activation = activation\n",
    "            self.activation_prime = activation_prime\n",
    "            self.activated = None\n",
    "            if kernel == None:\n",
    "                self.kernel = None\n",
    "            else:\n",
    "                self.kernel = kernel\n",
    "            if bias == None:\n",
    "                self.bias = None\n",
    "            else:\n",
    "                self.bias = bias\n",
    "            \n",
    "    def network_init(self, *kwargs):\n",
    "        for i in range(len(kwargs)):\n",
    "            if i != 0:\n",
    "                if kwargs[i].kernel == None:\n",
    "                    kwargs[i].kernel = np.random.rand(kwargs[i-1].num_nodes, kwargs[i].num_nodes)\n",
    "                if kwargs[i].bias == None:\n",
    "                    kwargs[i].bias = np.random.rand(kwargs[i].num_nodes)\n",
    "        return\n",
    "\n",
    "    \n",
    "    def fwd_prop(self, x, *kwargs):\n",
    "        assert x.ndim == 1\n",
    "        \n",
    "        for i in range(len(kwargs)):\n",
    "            if i == 0:\n",
    "                assert kwargs[i].num_nodes == len(x) \n",
    "                kwargs[i].activated = x\n",
    "            else:\n",
    "                z = np.dot(kwargs[i-1].activated, kwargs[i].kernel)# + kwargs[i].bias\n",
    "                kwargs[i].activated = kwargs[i].activation(z)\n",
    "        self.y_hat = kwargs[-1].activated\n",
    "        return\n",
    "    \n",
    "    def error(self, y):\n",
    "        assert self.y_hat.ndim == 1\n",
    "        assert self.y_hat.size == y.size\n",
    "        \n",
    "        sq_error = sum((y-self.y_hat)**2)\n",
    "        mse = sq_error/len(self.y_hat)\n",
    "        rmse = math.sqrt(sq_error)\n",
    "        \n",
    "        return {'sq_error': sq_error, 'mse': mse, 'rmse': rmse}\n",
    "    \n",
    "    def bwd_prop(self, y, step_size, *kwargs):\n",
    "        if step_size == None:\n",
    "            step_size = self.step_size\n",
    "        \n",
    "        # Remember that goal is to minimize error\n",
    "        # 1/2 used at the start is moot: the step size is scaled to appropriately deal with it\n",
    "        for i in np.arange(len(kwargs))[::-1]:\n",
    "            if i-1 >= 0:\n",
    "                delta = (y-self.y_hat)*kwargs[i].activation_prime(np.dot(kwargs[i-1].activated, kwargs[i].kernel))*(-1)\n",
    "                #print('size delta and pvs activation', delta.shape,  kwargs[i-1].activated.shape)\n",
    "                arrs = [kwargs[i-1].activated for j in range(delta.size)]\n",
    "                stacked_activations = np.stack(arrs)\n",
    "                dj_dk = np.dot(delta, stacked_activations)\n",
    "                kwargs[i].kernel = kwargs[i].kernel + np.reshape(dj_dk*step_size*(-1), (-1,1))\n",
    "        # XXX: can prolly find a way to make this code more compact\n",
    "        return\n",
    "    \n",
    "    def data_shuffle(self, x_train, y_train, indices_or_sections = None):\n",
    "        assert x_train.shape[0] == y_train.shape[0]\n",
    "        order = np.arange(x_train.shape[0])\n",
    "        np.random.shuffle(order)\n",
    "        x_shuffle = x_train[order]\n",
    "        y_shuffle = y_train[order]\n",
    "        \n",
    "        if indices_or_sections != None:\n",
    "            x_shuffle = np.split(x_shuffle, indices_or_sections)\n",
    "            y_shuffle = np.split(y_shuffle, indices_or_sections)\n",
    "        \n",
    "        return (x_shuffle, y_shuffle)\n",
    "    \n",
    "    def dimension_reducer(a):\n",
    "        shape = np.array(a.shape)\n",
    "        shape = shape[shape>1]\n",
    "        a.reshape(shape)\n",
    "        return a\n",
    "        \n",
    "    \n",
    "    def runner(self, x_train, y_train, indices_or_sections, x_test, y_test, layers):\n",
    "        x_splits, y_splits = self.data_shuffle(x_train, y_train, indices_or_sections = indices_or_sections)\n",
    "        \n",
    "        assert len(x_splits) == len(y_splits)\n",
    "        assert x_splits[0].shape[0] == y_splits[0].shape[0]\n",
    "        \n",
    "        effective_stepsize = self.step_size/(x_splits[0].shape[0])\n",
    "        \n",
    "        count = 0\n",
    "        for x,y in zip(x_splits, y_splits):\n",
    "            count += 1\n",
    "            for i in range(x.shape[0]):\n",
    "                self.fwd_prop(x[i], *layers)\n",
    "                self.bwd_prop(y[i], effective_stepsize, *layers)\n",
    "        sse = 0\n",
    "        for x,y in zip(x_test, y_test):\n",
    "            self.fwd_prop(x, *layers)\n",
    "            sse += self.error(y)['sq_error']\n",
    "        mse = sse/x_test.shape[0]\n",
    "        rmse = math.sqrt(mse)\n",
    "        \n",
    "        return {'sse':sse, 'mse':mse, 'rmse':rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_first_ann = neural_network(.1)\n",
    "my_first_ann.input_layer = my_first_ann.layer(13)\n",
    "my_first_ann.hidden_layer = my_first_ann.layer(26, activation = sigmoid, activation_prime = sigmoid_prime, kernel = None, bias = None)\n",
    "my_first_ann.output_layer = my_first_ann.layer(1, activation = linear, activation_prime = linear_prime, kernel = None, bias = None)\n",
    "my_first_ann.network_init(my_first_ann.input_layer, my_first_ann.hidden_layer, my_first_ann.output_layer)\n",
    "\n",
    "layers = [my_first_ann.input_layer, my_first_ann.hidden_layer, my_first_ann.output_layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.23247   0.        8.14      0.        0.538     6.142    91.7\n",
      "   3.9769    4.      307.       21.      396.9      18.72   ]\n",
      "[-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
      "  0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
      "  0.8252202 ]\n",
      "{'sse': 3074.7471437373097, 'mse': 30.14457984056186, 'rmse': 5.490407984891639}\n",
      "{'sse': 2798.1958841269375, 'mse': 27.43329298163664, 'rmse': 5.2376801144816625}\n",
      "[25.22136749]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(path=\"boston_housing.npz\", test_split=0.2, seed=113)\n",
    "\n",
    "print(x_train[0])\n",
    "\n",
    "x_train, x_test = z_score(x_train, x_test)\n",
    "\n",
    "print (x_train[0])\n",
    "\n",
    "for i in range(2):\n",
    "    errors = my_first_ann.runner(x_train, y_train, 1, x_test, y_test, layers)\n",
    "    print(errors)\n",
    "    print(my_first_ann.y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.88473919997341\n"
     ]
    }
   ],
   "source": [
    "# The sequential model is appropriate for models where the flow of information is linear: there is 1 input tensor at the start and 1 output tensor at the end\n",
    "# Abstraction of the functional API, where instead of each layer being a function, they are grouped together automatically\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(10, activation = \"relu\", input_shape = (13,), name = 'hidden'))\n",
    "    model.add(layers.Dense(1, name = 'output'))\n",
    "    # important to note that the default learn rate for keras rmsprop is .001. This can be changed by passing an rmsprop function with lr = something to the compiler.\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n",
    "    return model\n",
    "\n",
    "# Kfolding to determine number of times to train on the model. This is a method of settting hyperparameters without tuning them to the test dataset.\n",
    "k = 4\n",
    "size_val = len(x_train) // k\n",
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "all_histories = []\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for i in range(k):\n",
    "    x_val = x_train[size_val*i: size_val*(i+1)]\n",
    "    y_val = y_train[size_val*i: size_val*(i+1)]\n",
    "    x_partialTrain = np.concatenate([x_train[:i*size_val], x_train[(i+1)*size_val:]])\n",
    "    y_partialTrain = np.concatenate([y_train[:i*size_val], y_train[(i+1)*size_val:]])\n",
    "    \n",
    "    model = build_model()\n",
    "    # Note that the batch size is set to one. This results in each sample having a far greater effect on the weights.\n",
    "    history = model.fit(x_partialTrain, y_partialTrain, epochs = num_epochs, batch_size = 64, verbose = 0, validation_data = (x_val, y_val))\n",
    "    \n",
    "    all_histories.append(history.history)\n",
    "    all_mae_histories.append(history.history['val_mae'])\n",
    "end = timeit.default_timer()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.920958\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_mae_histories = np.array(all_mae_histories)\n",
    "all_mae_histories.shape\n",
    "mean_mae_hist = np.mean(all_mae_histories, axis = 0)\n",
    "plt.plot(np.arange(len(mean_mae_hist)), mean_mae_hist)\n",
    "print(np.min(mean_mae_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = build_model()\n",
    "print(x_train.shape, y_train.shape)\n",
    "model.fit(x_train, y_train, epochs = 80, batch_size = 16, verbose = 0)\n",
    "\n",
    "test_mse_score, test_mae_score = model.evaluate(x_test, y_test)\n",
    "print(test_mse_score,test_mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a simple Keras model\n",
    "* Keras is a framework of prexisting tools to easily create a model\n",
    "* Because it has already been developed for years, it has far more options that my custom model\n",
    "\n",
    "### Vocabulary\n",
    "* Optimizer: the method by which the gradient resulting from backpropogation is used to adjust the training weights\n",
    "* RMSprop (optimizer): The rmsprop optimization algorithm is similar to standard (linear) gradient descent, except that it calculates the RMS of the error and divides the squared error by that. The effect is to accentuate the areas with a higher gradient and decrease the effects of others\n",
    "* Loss function: the function that is differentiated durign the backpropogation process\n",
    "*  MSE (loss function): produces gradients that are independant of the number of samples. Effectively same as implemented in custom set up.\n",
    "* K-fold validation: a method of testing hyperparameters without actually using a test dataset. n each \"fold\" a fraction (1/k) of the training data is left out to act as a defacto test dataset. Then, a few fraction of the data is left out and the process repeats k times.\n",
    "* Epochs: the number of times the model is trained on the entire dataset\n",
    "* Batch size: the number of samples trained together. The higher the number of samples together (and for equal train rate), the less each sample contributes to learning, the more averaged the learning is. Smaller batches are often used because they increase learn rate and are more performative because less data is processed at once.\n",
    "* Stochastic gradient descent: gradient descent where the gradient is computed using a batch size is small. This is particularily useful when you have massive amounts of data and/or have a problem where you might accidentally get stuck in a local minimum. The stochastic part refers to how, because it is not an average, the gradient has a greater variance.\n",
    "\n",
    "### Batch Size vs Time\n",
    "* 1 vs 844.5401622999925 s\n",
    "* 16 vs 73.6799558000057 s\n",
    "* 64 vs 32.88473919997341 s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('tf': conda)",
   "language": "python",
   "name": "python37664bittfcondaf0a0b333407b4c12bfca13c48d28a585"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
