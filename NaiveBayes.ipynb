{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "### Bayes'  Theorem\n",
    "* Think about making a prediction, given a certain set of information\n",
    "* You have **Prior** beliefs, which is what you would assume without the data. Typically, this is the strict proportion of samples that have a certain category.\n",
    "* You have the **:ikelihood** function, which is the probability of observing the data given the observation. This is seperate from the posterior, \n",
    "* You have the **Normalization**, which is how likely you are to observe the data at all. You must divide this out because you want to prove that it is \"given\" the data.\n",
    "\n",
    ">$P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}$\n",
    "0. $P(Y|X) = Posterior$\n",
    "1. $P(Y) or P(Y|E)= Prior$\n",
    "2. $P(X|Y) = Likelihood$\n",
    "3. $P(D|E) = Normalization$\n",
    "\n",
    "### The \"Naive\" assumption\n",
    "* To be precise, $P(X|Y)$ might require that you already observed a case with the right mix of data. This might not be true when you have many features to your data which each have many options.\n",
    "* Instead, you can naively assume that all of the features of the data are unrelated. The result is that the likelihood can be expressed as the simple multiple of the probabilities of all features\n",
    "\n",
    "### Naive Bayes on a Natural Language Processing (NLP) Example\n",
    "* While Naive bayes is an excellent general purpose model, it is especially often used for natural language processing\n",
    "* Generally, the position of the words does not matter in natural language processing, thus each index where a word could be is not considered an individual feature.\n",
    "* Therefore, when calculating probabilities in NLP, you calculate it based on all of the data rather than just the data in a particular category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Resources</center>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "\n",
    "https://stats.stackexchange.com/questions/218492/how-does-naive-bayes-work-with-continuous-variables\n",
    "\n",
    "http://lib.stat.cmu.edu/datasets/boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model\n",
    "* Ended up being too slow due to training dataset values being calculated every time a test is run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import sklearn as skl\n",
    "\n",
    "# Additive smoothing (add-one smoothing aka Laplace Smoothing)\n",
    "# The key idea behind laplace smoothing is an assumption that you add one more sample to each value that a feature could recieve.\n",
    "# For example, say you are trying to calculate P(x=1); You would add 1 to the count of each value that x could take on.\n",
    "def laplace_smoothing(Num_trials_considered, fulfilling_x, Num_possibilities_x, a=1):\n",
    "    \"\"\"\n",
    "    The key idea behind laplace smoothing is an assumption that you add one more sample to each value that a feature could recieve. For example, say you are trying to calculate P(x=1); You would add 1 to the count of each value that x could take on.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Num_trials_considered : int\n",
    "        number of trials being looked at (same as number of observations). Bounded by a particular y valu. Can be expressed as len(y[y==y_intended])\n",
    "    Num_possibilities_x : int\n",
    "        number of posibilites that the specified feature could take on.\n",
    "    fulfilling_x : int\n",
    "        the number of trials (out of N) where the specified feature attains a specific value. THe sum of all possible values of fulfilling_x (ie, sum of fulfilling_x for all values of the specified feature) should be equal to k.\n",
    "    a : int, default=1\n",
    "        the addition constant. Represents the number of extra observations added to each value of a feature.\n",
    "    \"\"\"\n",
    "    # Typically we would just take x/N\n",
    "    lp = (fulfilling_x + a) / (Num_trials_considered + a * Num_possibilities_x)\n",
    "    \n",
    "    return lp\n",
    "# Naive bayes expressed in code\n",
    "def naive_bayes(y, x, categories, data, lp_smoothing = True):\n",
    "    y = np.array(y)\n",
    "    x = np.array(x)\n",
    "    \n",
    "    posteriors = {}\n",
    "    norm_likelihood_all = []\n",
    "    prior_all = []\n",
    "    \n",
    "    num_samples = x.shape[0]\n",
    "    num_features = x.shape[1]\n",
    "    \n",
    "    for category in categories:\n",
    "        \n",
    "        given_cat = (y == category)\n",
    "        if lp_smoothing:\n",
    "            prior = laplace_smoothing(Num_trials_considered = num_samples, fulfilling_x = np.count_nonzero(given_cat), Num_possibilities_x = len(categories), a = 1)\n",
    "        else:\n",
    "            prior = np.count_nonzero(given_cat)/len(y)\n",
    "        \n",
    "        x_given_cat = x[given_cat]\n",
    "        norm_likelihood = 1\n",
    "        for i in range(len(data)):\n",
    "            feature_likelihood = x_given_cat[:, i]\n",
    "            feature_normalization = x[:, i]\n",
    "            if lp_smoothing:\n",
    "                if norm_likelihood == 0:\n",
    "                    #print(i)\n",
    "                    pass\n",
    "                norm_likelihood *= laplace_smoothing(Num_trials_considered = np.count_nonzero(given_cat), fulfilling_x = np.count_nonzero(feature_likelihood == data[i]), Num_possibilities_x = 10000, a = 1) / laplace_smoothing(Num_trials_considered = num_samples, fulfilling_x = np.count_nonzero(feature_normalization == data[i]), Num_possibilities_x = num_features, a = 1)\n",
    "            else:\n",
    "                norm_likelihood *= np.count_nonzero(feature == data[i])/np.count_nonzero(given_cat) / np.count_nonzero(feature == data[i])/np.count_nonzero(y)\n",
    "                \n",
    "        prior_all.append(prior)\n",
    "        norm_likelihood_all.append(norm_likelihood)\n",
    "        \n",
    "        posteriors[category] = prior*norm_likelihood\n",
    "    return (posteriors, prior_all, norm_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New, memory efficient version\n",
    "\n",
    "class naive_b():\n",
    "        \n",
    "    def train(self,x_train,y_train,verbose = True):\n",
    "        assert x_train.ndim == 2\n",
    "        \n",
    "        vocab_size = x_train.shape[1]\n",
    "        # Assume classes are sequential integers starting at 1\n",
    "        num_classes = int(max(y_train)) + 1\n",
    "        \n",
    "        normalizations = np.sum(x_train, axis = 0)\n",
    "        normalizations /= np.sum(normalizations)\n",
    "        \n",
    "        priors = np.zeros(num_classes,)\n",
    "        likelihoods = np.zeros((num_classes, vocab_size))\n",
    "        \n",
    "        for i in range(num_classes):\n",
    "            y_class = (y_train == i)\n",
    "            class_count = np.count_nonzero(y_class)\n",
    "            priors[i,] = class_count/len(y_train)\n",
    "            \n",
    "            x_class = x_train[y_class]\n",
    "            likelihoods[i,:] = (np.sum(x_class, axis = 0) + 1) / (class_count + vocab_size)\n",
    "        self.priors = priors\n",
    "        self.likelihoods = likelihoods\n",
    "        self.normalizations = normalizations\n",
    "    \n",
    "    def test(self,x_test):\n",
    "        posteriors_all = []\n",
    "        preds = []\n",
    "        for i in range(len(x_test)):\n",
    "            likelihood = np.prod(self.likelihoods[:, x_test[i] != 0], axis = 1)\n",
    "            normalization = (self.normalizations*x_test[i])\n",
    "            normalization = np.prod(normalization[normalization != 0])\n",
    "            posteriors = (self.priors * likelihood) / normalization\n",
    "            \n",
    "            posteriors_all.append(posteriors)\n",
    "            preds.append(np.argmax(posteriors))\n",
    "        return preds, posteriors_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently the data comes in the form of variable length lists (e.g., below). We want to make everything the same length to make it computer friendly\n",
      "len ex. 1: 87 len ex. 2: 56\n"
     ]
    }
   ],
   "source": [
    "# Trying to use Naive bayes with Reuters dataset\n",
    "\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vocab_size = 500\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words = vocab_size)\n",
    "\n",
    "print('Currently the data comes in the form of variable length lists (e.g., below). We want to make everything the same length to make it computer friendly')\n",
    "print('len ex. 1:', len(train_data[0]), 'len ex. 2:', len(train_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 500) (500,)\n"
     ]
    }
   ],
   "source": [
    "# An example of what a simple vectorizer might do\n",
    "# def vectorizer(sequences, dimension=500):\n",
    "#     # Convert lists of different sizes to boolean vectors with length equal to the total number of words\n",
    "#     vectorized = np.zeros((len(sequences), dimension))\n",
    "#     for i, sequence in enumerate(sequences):\n",
    "#         vectorized[i, sequence] = 1\n",
    "#     return vectorized\n",
    "\n",
    "def count_vectorizer(list_of_lists, vocab_size):\n",
    "    vectorized_sequences= []\n",
    "    for sequence in list_of_lists:\n",
    "        unique, counts = np.unique(np.array(sequence), return_counts = True)\n",
    "        vectorized = np.zeros(vocab_size)\n",
    "        vectorized[unique] = counts\n",
    "        vectorized_sequences.append(vectorized)\n",
    "    out = np.stack(vectorized_sequences, axis = 0)\n",
    "    return out\n",
    "\n",
    "x_train = count_vectorizer(train_data, vocab_size)\n",
    "x_test = count_vectorizer(test_data, vocab_size)\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "print(x_train.shape, x_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.5952804986642921\n",
      "random guess:  0.022222222222222223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:36: RuntimeWarning: divide by zero encountered in true_divide\n",
      "C:\\Users\\HP\\Anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:36: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "model = naive_b()\n",
    "model.train(x_train, y_train)\n",
    "preds, posteriors = model.test(x_test)\n",
    "# Relatively high accuracy\n",
    "print('accuracy: ', np.count_nonzero(y_test == preds) / len(y_test))\n",
    "print('random guess: ', 1/(max(y_train) - min(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7141585040071238\n",
      "random guess:  0.022222222222222223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "sk_model = MultinomialNB()\n",
    "sk_model.fit(x_train, y_train)\n",
    "sk_preds = sk_model.predict(x_test)\n",
    "# Relatively high accuracy\n",
    "print('accuracy: ', np.count_nonzero(y_test == sk_preds) / len(y_test))\n",
    "print('random guess: ', 1/(max(y_train) - min(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Boston Housing\n",
    "* Discretizing the boston housing dataset is hard because there are too few features\n",
    "\n",
    "#### Boston Housing Dataset Data Lables\n",
    "* CRIM     per capita crime rate by town\n",
    "* ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* INDUS    proportion of non-retail business acres per town\n",
    "* CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "* NOX      nitric oxides concentration (parts per 10 million)\n",
    "* RM       average number of rooms per dwelling\n",
    "* AGE      proportion of owner-occupied units built prior to 1940\n",
    "* DIS      weighted distances to five Boston employment centres\n",
    "* RAD      index of accessibility to radial highways\n",
    "* TAX      full-value property-tax rate per \\$10,000\n",
    "* PTRATIO  pupil-teacher ratio by town\n",
    "* B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "* LSTAT    % lower status of the population\n",
    "* MEDV     Median value of owner-occupied homes in \\$1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13) (102, 13)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "import pandas as pd\n",
    "\n",
    "(bh_x_train, bh_y_train), (bh_x_test, bh_y_test) = boston_housing.load_data()\n",
    "\n",
    "print(bh_x_train.shape, bh_x_test.shape)\n",
    "\n",
    "# Normalization routine\n",
    "def z_score(x_train, x_test):\n",
    "    # Note that all transformation on the data are based on the training data\n",
    "    # The goal is to make sure that you don't accidentally train on any aspects of the test data\n",
    "    mean = x_train.mean(axis = 0)\n",
    "    std = x_train.std(axis = 0)\n",
    "    \n",
    "    x_train_norm = (x_train - mean) / std\n",
    "    x_test_norm = (x_test - mean) / std\n",
    "    \n",
    "    return (x_train_norm, x_test_norm, mean, std)\n",
    "\n",
    "bh_x_train, bh_x_test, mean, std = z_score(bh_x_train, bh_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\HP\\Anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "descretizer_x = KBinsDiscretizer(n_bins = 3, encode = 'onehot-dense', strategy='quantile')\n",
    "\n",
    "print(bh_x_train.shape)\n",
    "bh_x_train = descretizer_x.fit_transform(bh_x_train)\n",
    "bh_x_test = descretizer_x.transform(bh_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0392156862745098\n"
     ]
    }
   ],
   "source": [
    "# This predictor has a much lower success rate because we do not have the same number of features and categories within features.\n",
    "\n",
    "bh_model = naive_b()\n",
    "bh_model.train(bh_x_train, bh_y_train)\n",
    "bh_preds, bh_posteriors = bh_model.test(bh_x_test)\n",
    "print(np.count_nonzero(bh_y_test == bh_preds) / len(bh_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Type of Data\n",
    "* Data that can be counted and can be probabalized (e.g., word data)\n",
    "  * Discrete data\n",
    "\n",
    "### 2. Use Case\n",
    "* Natural Language Processing\n",
    "* Multiclass identification\n",
    "\n",
    "### 3. Basic Concept\n",
    "* Use Bayes' theorem to identify the probability of each example belonging to each class.\n",
    "* The naive assumption is that the features are unrelated\n",
    "* The class with the highest probability is used\n",
    "\n",
    "### 4. Assumptions\n",
    "* Words are unrelated\n",
    "* Bag-of-words is effective\n",
    "\n",
    "### 5. Application\n",
    "* NLP is particularily well suited \n",
    "\n",
    "### 6. Existing solutions\n",
    "* SKlearn\n",
    "\n",
    "### 7. Strengths and Weaknesses\n",
    "#### Strengths\n",
    "* Simple, easy to understand\n",
    "* Given sufficient data, can be very effective\n",
    "\n",
    "#### Weaknesses\n",
    "* Can be highly memory intensive during training\n",
    "* Needs special techniques to make sure underflows do not happen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('tf2': conda)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
